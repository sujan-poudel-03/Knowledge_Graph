{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ad79c0",
   "metadata": {},
   "source": [
    "# 1. Scrape text data from some selected articles from above link. You can use beautiful soup, newspapers or any data scraping libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL of the website to scrape\n",
    "url = \"https://english.onlinekhabar.com/\"\n",
    "\n",
    "# Send a GET request to the website and retrieve the HTML content\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Find the relevant articles or sections on the webpage\n",
    "articles = soup.find_all(\"article\")\n",
    "\n",
    "# Loop through the articles and extract the text content\n",
    "scraped_text = \"\"\n",
    "for article in articles:\n",
    "    # Extract the text from the article\n",
    "    text = article.get_text(separator=\" \")\n",
    "\n",
    "    # Append the extracted text to the overall scraped text\n",
    "    scraped_text += text + \"\\n\"\n",
    "\n",
    "# Print the scraped text\n",
    "print(scraped_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562263e",
   "metadata": {},
   "source": [
    "# 2. Process text for sentences and apply necessary NLP processing. You can use nltk , spacy or any NLP libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c47d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\FCT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\FCT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\FCT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence .\n",
      "showcase nlp processing .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Example text\n",
    "text = \"This is an example sentence. It showcases NLP processing.\"\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Process each sentence\n",
    "processed_sentences = []\n",
    "for sentence in sentences:\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Remove stop words and convert to lowercase\n",
    "    words = [word.lower() for word in words if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Join the lemmatized words back into a sentence\n",
    "    processed_sentence = \" \".join(lemmatized_words)\n",
    "\n",
    "    # Append the processed sentence to the list\n",
    "    processed_sentences.append(processed_sentence)\n",
    "\n",
    "# Print the processed sentences\n",
    "for sentence in processed_sentences:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a69023d",
   "metadata": {},
   "source": [
    "# 3. Extract the subject, object and relationship from each sentence. Extracting entities with modifiers is a plus point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "098dd1bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name dataclass_transform",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4656/2172471032.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Load the English language model in spaCy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# set library-specific custom warning handling before doing anything else\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# noqa: E402\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\errors.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mErrorsWithCodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\compat.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopy_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\config.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcatalogue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mconfection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mconfection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConfigValidationError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPromise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVARIABLE_RE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecorator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\confection\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mconfigparser\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mParsingError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExtra\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModelMetaclass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfields\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModelField\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\__init__.cp39-win_amd64.pyd\u001b[0m in \u001b[0;36minit pydantic.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\dataclasses.cp39-win_amd64.pyd\u001b[0m in \u001b[0;36minit pydantic.dataclasses\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name dataclass_transform"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"Apple Inc. is headquartered in Cupertino, California.\"\n",
    "\n",
    "# Process the sentence using spaCy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extract the subject, object, and relationship\n",
    "subject = \"\"\n",
    "object_ = \"\"\n",
    "relationship = \"\"\n",
    "\n",
    "# Iterate over the sentence tokens\n",
    "for token in doc:\n",
    "    # Extract the subject\n",
    "    if \"subj\" in token.dep_:\n",
    "        subject = token.text\n",
    "\n",
    "    # Extract the object\n",
    "    if \"obj\" in token.dep_:\n",
    "        object_ = token.text\n",
    "\n",
    "    # Extract the relationship\n",
    "    if \"ROOT\" in token.dep_:\n",
    "        relationship = token.text\n",
    "\n",
    "# Print the extracted subject, object, and relationship\n",
    "print(\"Subject:\", subject)\n",
    "print(\"Object:\", object_)\n",
    "print(\"Relationship:\", relationship)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c49f2a",
   "metadata": {},
   "source": [
    "# 4. Build a directed graph from the above data with entities as a node and relationships as an edge. Label each node and edge with corresponding texts. You can use nexworkx, arongodb, neo4j or any graph/network library.Save the graph db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b974ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7768fdac",
   "metadata": {},
   "source": [
    "# 5. Get an answer to the given question. Question sentences should be in natural language, follow the necessary steps to get the answer from the graph db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ff386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f0cbafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install BeautifulSoup4\n",
    "# !pip install newspaper3k\n",
    "# !pip install pymongo\n",
    "# !pip install nltk\n",
    "# !pip install networkx\n",
    "# !pip install spacy\n",
    "# !pip install py2neo\n",
    "\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f04130",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import re\n",
    "\n",
    "# Step 1: Scraping text data from selected articles\n",
    "url = \"https://english.onlinekhabar.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# sections = soup.select(\"section.ok-news-grid\")[:3]\n",
    "# print(sections)\n",
    "\n",
    "# For all categories \n",
    "# for section in sections:\n",
    "#     section_detail = section.select_one(\"h2 a\")\n",
    "#     if section_detail:\n",
    "#         title = section_detail.text.strip()\n",
    "#         link = section_detail[\"href\"]\n",
    "#         print(\"----\")\n",
    "#         print(\"Title:\", title)\n",
    "#         print(\"Link:\", link)\n",
    "\n",
    "# print(\"----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b855cd8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▏                                                                                | 2/76 [00:00<00:07,  9.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▍                                                                             | 5/76 [00:00<00:08,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███████▋                                                                           | 7/76 [00:00<00:08,  7.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████▊                                                                         | 9/76 [00:01<00:08,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████▊                                                                      | 11/76 [00:01<00:07,  8.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|████████████▉                                                                     | 12/76 [00:01<00:07,  8.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████████████                                                                   | 14/76 [00:01<00:09,  6.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|█████████████████▎                                                                | 16/76 [00:02<00:08,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|███████████████████▍                                                              | 18/76 [00:02<00:07,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████████████████▋                                                           | 21/76 [00:02<00:06,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████████████▉                                                        | 24/76 [00:03<00:06,  8.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████████████████████                                                      | 26/76 [00:03<00:06,  7.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████████████████████▏                                                   | 28/76 [00:03<00:06,  8.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|████████████████████████████████▎                                                 | 30/76 [00:03<00:05,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|██████████████████████████████████▌                                               | 32/76 [00:04<00:06,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████████████████████████████▋                                             | 34/76 [00:04<00:05,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|██████████████████████████████████████▊                                           | 36/76 [00:04<00:04,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|███████████████████████████████████████▉                                          | 37/76 [00:04<00:04,  8.35it/s]"
     ]
    }
   ],
   "source": [
    "# Content of Home page\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Scraping text data from selected articles\n",
    "url = \"https://english.onlinekhabar.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "sections = soup.select(\"div.ok-news-post\")\n",
    "# print(sections)\n",
    "\n",
    "links = []\n",
    "\n",
    "for section in sections:\n",
    "    section_detail = section.select_one(\"h2 a\" )\n",
    "#     print(section_detail)\n",
    "    link = section_detail.get('href')\n",
    "#     print(link)\n",
    "    links.append(link)\n",
    "    \n",
    "# print(links)\n",
    "\n",
    "news_dict = {}\n",
    "def inside_content(links_list):\n",
    "    for i in tqdm(links_list):   \n",
    "#         print(i)\n",
    "        response = requests.get(i)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        title = soup.select_one(\"h1\").text\n",
    "        author = soup.select_one(\"div.ok-author span\").text \n",
    "        post_date = soup.select_one(\"span.ok-post-date\").text\n",
    "        \n",
    "        div_element = soup.find(\"div\", class_=\"post-content-wrap\")\n",
    "        paragraphs = div_element.find_all(\"p\")\n",
    "        content = \"\"\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            content = content + paragraph.get_text()\n",
    "\n",
    "        \n",
    "#         print(f\"Author: {author}\")\n",
    "#         print(f\"Date: {post_date}\")\n",
    "#         print(f\"Title: {title}\")\n",
    "#         print(f\"Content: {content}\")\n",
    "        \n",
    "        news_dict = {\n",
    "            \"Link\" : i,\n",
    "            \"Author\": author,\n",
    "            \"Date\" : post_date,\n",
    "            \"Title\" : title,\n",
    "            \"Content\" : content\n",
    "        }\n",
    "        \n",
    "        \n",
    "inside_content(links)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff55e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No news found on News Politics.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Applying necessary NLP processing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Step 3: Extracting subject, object, and relationship from each sentence\n",
    "edges = []\n",
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        if token.dep_ == \"nsubj\" and token.head.pos_ == \"VERB\":\n",
    "            subject = token.text\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"amod\":\n",
    "                    subject = child.text + \" \" + subject\n",
    "            for child in token.head.children:\n",
    "                if child.dep_ == \"dobj\":\n",
    "                    obj = child.text\n",
    "                    for grandchild in child.children:\n",
    "                        if grandchild.dep_ == \"amod\":\n",
    "                            obj = grandchild.text + \" \" + obj\n",
    "                    edges.append((subject, obj, token.head.text))\n",
    "\n",
    "# Step 4: Building a directed graph from the above data\n",
    "G = nx.DiGraph()\n",
    "for edge in edges:\n",
    "    G.add_edge(edge[0], edge[1], relation=edge[2])\n",
    "\n",
    "# Step 5: Geting an answer to the given question\n",
    "question = \"What is the latest news on politics?\"\n",
    "parsed_question = nlp(question)\n",
    "\n",
    "query = \"\"\n",
    "for token in parsed_question:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        query += f\"{token.text.title()} \"\n",
    "    elif token.pos_ == \"PROPN\":\n",
    "        query += f\"{token.text} \"\n",
    "\n",
    "if query:\n",
    "    answers = []\n",
    "    for edge in G.edges(data=True):\n",
    "        if query.strip() in (edge[0], edge[1]):\n",
    "            answers.append(edge[2][\"relation\"])\n",
    "    if answers:\n",
    "        print(f\"The latest news on {query.strip()} is: {max(answers)}\")\n",
    "    else:\n",
    "        print(f\"No news found on {query.strip()}.\")\n",
    "else:\n",
    "    print(\"Invalid question format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b70ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ec9501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
